{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b154be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset (data.csv) link: https://huggingface.co/datasets/AmirMohseni/GroceryList\n",
    "# final_grocery_dataset.csv contains all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# print(keras.__version__)\n",
    "# keras.backend.backend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccae86c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1385\n"
     ]
    }
   ],
   "source": [
    "# basic cleaning \n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# load dataset\n",
    "final_df = pd.read_csv(\"data/final_grocery_dataset.csv\", names=[\"Item\", \"Category\"])\n",
    "\n",
    "# clean text\n",
    "final_df[\"Item\"] = final_df[\"Item\"].apply(clean_text)\n",
    "final_df[\"Category\"] = final_df[\"Category\"].str.strip()\n",
    "\n",
    "# remove dupes\n",
    "final_df = final_df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "\n",
    "# print(df[\"Category\"].value_counts())\n",
    "# print(\"Total rows:\", len(df))\n",
    "\n",
    "print(len(final_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78dd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "Other                  130\n",
      "Snacks                 130\n",
      "Bakery                 130\n",
      "Pet Supplies           130\n",
      "Dairy & Eggs           108\n",
      "Beverages               93\n",
      "Condiments & Sauces     89\n",
      "Produce                 62\n",
      "Meat & Seafood          43\n",
      "Frozen Foods            36\n",
      "Household               35\n",
      "Canned Goods            32\n",
      "Pasta & Grains          21\n",
      "Pantry                  18\n",
      "Personal Care           16\n",
      "Deli                    13\n",
      "Category                 1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "CAP = 130\n",
    "\n",
    "limited_dfs = []\n",
    "\n",
    "for category in final_df[\"Category\"].unique():\n",
    "    limited_dfs.append(\n",
    "        final_df[final_df[\"Category\"] == category].head(CAP)\n",
    "    )\n",
    "\n",
    "final_df = pd.concat(limited_dfs, ignore_index=True)\n",
    "\n",
    "print(final_df[\"Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653faabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge smaller categories \n",
    "\n",
    "category_merge_map = {\n",
    "    \"Condiments & Sauces\": \"Pantry Items\",\n",
    "    \"Canned Goods\": \"Pantry Items\",\n",
    "    \"Pantry\": \"Pantry Items\",\n",
    "\n",
    "    \"Pasta & Grains\": \"Grains & Bakery\",\n",
    "    \"Bakery\": \"Grains & Bakery\",\n",
    "\n",
    "    \"Deli\": \"Meat & Deli\",\n",
    "    \"Meat & Seafood\": \"Meat & Deli\",\n",
    "}\n",
    "\n",
    "final_df[\"Category\"] = final_df[\"Category\"].replace(category_merge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839effb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 12\n",
      "{'Beverages': 0, 'Dairy & Eggs': 1, 'Frozen Foods': 2, 'Grains & Bakery': 3, 'Household': 4, 'Meat & Deli': 5, 'Other': 6, 'Pantry Items': 7, 'Personal Care': 8, 'Pet Supplies': 9, 'Produce': 10, 'Snacks': 11}\n"
     ]
    }
   ],
   "source": [
    "# encode labels - used chat gpt to help write this cell\n",
    "\n",
    "# df = final_df.copy()\n",
    "\n",
    "final_df = final_df[final_df[\"Category\"] != \"Category\"].reset_index(drop=True)\n",
    "\n",
    "# encode labels\n",
    "categories = sorted(final_df[\"Category\"].unique().tolist())\n",
    "\n",
    "cat_to_idx = {}\n",
    "for idx, category in enumerate(categories):\n",
    "    cat_to_idx[category] = idx\n",
    "\n",
    "idx_to_cat = {}\n",
    "for category, idx in cat_to_idx.items():\n",
    "    idx_to_cat[idx] = category\n",
    "\n",
    "final_df[\"label\"] = final_df[\"Category\"].map(cat_to_idx)\n",
    "\n",
    "print(\"Number of categories:\", len(categories))\n",
    "print(cat_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a555aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/text split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_df[\"Item\"].values,\n",
    "    final_df[\"label\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=final_df[\"label\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e5da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vectoriser\n",
    "max_tokens = 5000\n",
    "sequence_length = 20\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=5000,\n",
    "    ngrams = 2,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 20)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 20, 64)            320000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 64)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 324940 (1.24 MB)\n",
      "Trainable params: 324940 (1.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model - used chatgpt to help with the syntax\n",
    "\n",
    "num_classes = len(categories)\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=max_tokens, output_dim=64)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feaf91e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2.4636 - accuracy: 0.1628 - val_loss: 2.4450 - val_accuracy: 0.2069\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.4009 - accuracy: 0.2190 - val_loss: 2.3927 - val_accuracy: 0.2126\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.3211 - accuracy: 0.2594 - val_loss: 2.3336 - val_accuracy: 0.2356\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.2371 - accuracy: 0.2795 - val_loss: 2.2517 - val_accuracy: 0.2816\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1377 - accuracy: 0.3112 - val_loss: 2.1546 - val_accuracy: 0.2931\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0402 - accuracy: 0.3228 - val_loss: 2.0758 - val_accuracy: 0.2874\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9767 - accuracy: 0.3458 - val_loss: 2.0272 - val_accuracy: 0.2759\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9178 - accuracy: 0.3991 - val_loss: 1.9838 - val_accuracy: 0.3621\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8624 - accuracy: 0.4654 - val_loss: 1.9385 - val_accuracy: 0.3736\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7966 - accuracy: 0.5216 - val_loss: 1.8853 - val_accuracy: 0.4138\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7129 - accuracy: 0.5418 - val_loss: 1.8242 - val_accuracy: 0.4425\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6172 - accuracy: 0.6196 - val_loss: 1.7596 - val_accuracy: 0.4885\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4980 - accuracy: 0.6599 - val_loss: 1.6854 - val_accuracy: 0.5115\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3933 - accuracy: 0.6916 - val_loss: 1.6010 - val_accuracy: 0.5632\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2814 - accuracy: 0.7147 - val_loss: 1.5422 - val_accuracy: 0.5632\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1651 - accuracy: 0.7406 - val_loss: 1.4782 - val_accuracy: 0.5805\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0695 - accuracy: 0.7579 - val_loss: 1.4085 - val_accuracy: 0.5977\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9795 - accuracy: 0.7608 - val_loss: 1.3658 - val_accuracy: 0.5977\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.9078 - accuracy: 0.7695 - val_loss: 1.3305 - val_accuracy: 0.6092\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.8287 - accuracy: 0.7983 - val_loss: 1.2892 - val_accuracy: 0.6034\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d50d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 749us/step - loss: 1.3729 - accuracy: 0.5872\n",
      "Test accuracy: 0.5871559381484985\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for overfitting - use chatgpt to help write and understand this correctly\n",
    "\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"train acc\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"val acc\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b75e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Other', 0.38576292991638184)\n",
      "('Other', 0.21808284521102905)\n",
      "('Grains & Bakery', 0.7206591367721558)\n",
      "('Other', 0.17957228422164917)\n",
      "('Other', 0.23095408082008362)\n",
      "('Other', 0.18567687273025513)\n",
      "('Pet Supplies', 0.7762845158576965)\n"
     ]
    }
   ],
   "source": [
    "# this is in the interface file (only here for testing) \n",
    "\n",
    "# def predict_category(item_name: str, threshold=0.5):\n",
    "#     item_name = clean_text(item_name)\n",
    "\n",
    "#     x = tf.constant([[item_name]])\n",
    "#     probs = model.predict(x, verbose=0)[0]\n",
    "\n",
    "#     max_prob = float(probs.max())\n",
    "#     pred_idx = int(probs.argmax())\n",
    "\n",
    "#     if max_prob < threshold:\n",
    "#         return \"Other\", max_prob\n",
    "\n",
    "#     return idx_to_cat[pred_idx], max_prob\n",
    "\n",
    "\n",
    "# print(predict_category(\"oat milk\"))\n",
    "# print(predict_category(\"chocolate biscuits\"))\n",
    "# print(predict_category(\"sourdough bread\"))\n",
    "# print(predict_category(\"toothpaste\"))\n",
    "# print(predict_category(\"oil\"))\n",
    "# print(predict_category(\"chicken\"))\n",
    "# print(predict_category(\"dry cat food\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8dafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"model.keras\")\n",
    "\n",
    "# save label map\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump({int(k): v for k, v in idx_to_cat.items()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task3 environment\n",
    "\n",
    "# model.save(\"task1_model_task3_env.keras\")\n",
    "\n",
    "# with open(\"task1_label_map_task3_env.json\", \"w\") as f:\n",
    "#     json.dump(idx_to_cat, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding-five-task3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
