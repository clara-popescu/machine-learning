{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c144720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import HTTPError, RequestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac3f478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex + cleaning helpers\n",
    "\n",
    "unit_re = re.compile(\n",
    "    r\"\\b(\\d+(\\.\\d+)?\\s?(g|kg|ml|l|cl|oz|lb|pack|pcs|pc|x))\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "multispace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def clean_name(s: str) -> str:\n",
    "    # normalise product names so they look like shopping-list items.\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = unit_re.sub(\"\", s)\n",
    "    # remove most punctuation but keep &, ' and -\n",
    "    s = re.sub(r\"[^\\w\\s&'-]\", \" \", s)\n",
    "    s = multispace_re.sub(\" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5ca42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories to scrape\n",
    "TARGET_CATEGORIES = [\n",
    "    \"Produce\",\n",
    "    \"Meat & Seafood\",\n",
    "    \"Dairy & Eggs\",\n",
    "    \"Bakery\",\n",
    "    \"Pantry\",\n",
    "    \"Frozen Foods\",\n",
    "    \"Beverages\",\n",
    "    \"Snacks\",\n",
    "    \"Personal Care\",\n",
    "    \"Household\",\n",
    "    \"Pet Supplies\",\n",
    "    \"Deli\",\n",
    "    \"Condiments & Sauces\",\n",
    "    \"Canned Goods\",\n",
    "    \"Pasta & Grains\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "\n",
    "def normalise_category(cat: str) -> str:\n",
    "    if not cat:\n",
    "        return \"Other\"\n",
    "    c = cat.strip().lower()\n",
    "    mapping = {\n",
    "        \"produce\": \"Produce\",\n",
    "        \"meat & seafood\": \"Meat & Seafood\",\n",
    "        \"dairy & eggs\": \"Dairy & Eggs\",\n",
    "        \"bakery\": \"Bakery\",\n",
    "        \"pantry\": \"Pantry\",\n",
    "        \"frozen foods\": \"Frozen Foods\",\n",
    "        \"beverages\": \"Beverages\",\n",
    "        \"snacks\": \"Snacks\",\n",
    "        \"personal care\": \"Personal Care\",\n",
    "        \"household\": \"Household\",\n",
    "        \"pet supplies\": \"Pet Supplies\",\n",
    "        \"deli\": \"Deli\",\n",
    "        \"condiments & sauces\": \"Condiments & Sauces\",\n",
    "        \"canned goods\": \"Canned Goods\",\n",
    "        \"pasta & grains\": \"Pasta & Grains\",\n",
    "        \"other\": \"Other\",\n",
    "    }\n",
    "    return mapping.get(c, \"Other\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea92b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open food facts (off) -> manual category mapping -- used ChatGPT to help generate this list\n",
    "off_to_manual_category = {\n",
    "    \"produce\": [\n",
    "        \"en:fruits\", \"en:vegetables\", \"en:fruit\", \"en:vegetable\",\n",
    "        \"en:produce\", \"en:salads\", \"en:herbs\"\n",
    "    ],\n",
    "    \"dairy & eggs\": [\n",
    "        \"en:dairies\", \"en:dairy\", \"en:milk-and-yogurt\", \"en:cheeses\",\n",
    "        \"en:yogurts\", \"en:eggs\", \"en:butter\", \"en:cream\"\n",
    "    ],\n",
    "    \"meat & seafood\": [\n",
    "        \"en:meats\", \"en:meat\", \"en:poultry\",\n",
    "        \"en:sausages\", \"en:fish-and-seafood\"\n",
    "    ],\n",
    "    \"bakery\": [\n",
    "        \"en:breads\", \"en:bread\", \"en:bakery-products\",\n",
    "        \"en:cakes\", \"en:biscuits\", \"en:pastries\"\n",
    "    ],\n",
    "    \"snacks\": [\n",
    "        \"en:snacks\", \"en:salty-snacks\", \"en:crisps\",\n",
    "        \"en:chips\", \"en:snack-foods\", \"en:confectioneries\"\n",
    "    ],\n",
    "    \"beverages\": [\n",
    "        \"en:beverages\", \"en:drinks\", \"en:soft-drinks\",\n",
    "        \"en:juices\", \"en:teas\", \"en:coffees\"\n",
    "    ],\n",
    "    \"canned goods\": [\n",
    "        \"en:canned-foods\", \"en:canned-vegetables\",\n",
    "        \"en:canned-fruits\", \"en:canned-fish\"\n",
    "    ],\n",
    "    \"condiments & sauces\": [\n",
    "        \"en:condiments\", \"en:sauces\", \"en:ketchups\", \"en:mustards\",\n",
    "        \"en:mayonnaises\", \"en:salad-dressings\"\n",
    "    ],\n",
    "    \"pasta & grains\": [\n",
    "        \"en:pasta\", \"en:rices\", \"en:cereals\", \"en:flours\",\n",
    "        \"en:grains\"\n",
    "    ],\n",
    "    \"frozen foods\": [\n",
    "        \"en:frozen-foods\", \"en:frozen\", \"en:ice-creams\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def map_off_to_category(off_tags):\n",
    "    off_tags = set(off_tags or [])\n",
    "    for manual_cat, off_cats in off_to_manual_category.items():\n",
    "        if any(tag in off_tags for tag in off_cats):\n",
    "            return manual_cat\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deca0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape products from Open Food Facts\n",
    "def fetch_off_page(country: str, page: int, page_size: int = 1000):\n",
    "    url = \"https://uk.openfoodfacts.org/cgi/search.pl\"\n",
    "    params = {\n",
    "        \"search_simple\": 1,\n",
    "        \"action\": \"process\",\n",
    "        \"json\": 1,\n",
    "        \"page\": page,\n",
    "        \"page_size\": page_size,\n",
    "        \"fields\": (\n",
    "            \"product_name,product_name_en,\"\n",
    "            \"generic_name,generic_name_en,\"\n",
    "            \"categories_tags,brands,quantity\"\n",
    "        ),\n",
    "        \"country\": country,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def build_off_dataset(country=\"united-kingdom\", pages=10, page_size=500, sleep_s=1.0) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    seen = set()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        data = fetch_off_page(country=country, page=page, page_size=page_size)\n",
    "        products = data.get(\"products\", [])\n",
    "\n",
    "        for p in products:\n",
    "            name = (\n",
    "                p.get(\"product_name_en\")\n",
    "                or p.get(\"product_name\")\n",
    "                or p.get(\"generic_name_en\")\n",
    "                or p.get(\"generic_name\")\n",
    "                or \"\"\n",
    "            )\n",
    "            name_clean = clean_name(name)\n",
    "            if len(name_clean) < 3:\n",
    "                continue\n",
    "            if name_clean in seen:\n",
    "                continue\n",
    "            seen.add(name_clean)\n",
    "\n",
    "            manual_cat = map_off_to_category(p.get(\"categories_tags\", []))\n",
    "            cat = normalise_category(manual_cat)\n",
    "            rows.append({\"Item\": name_clean, \"Category\": cat})\n",
    "\n",
    "        print(\n",
    "            f\"[OFF] Page {page}/{pages}: +{len(products)} products, \"\n",
    "            f\"dataset size now {len(rows)}\"\n",
    "        )\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc187bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supermarket scraping helpers\n",
    "WEB_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_titles(url: str,\n",
    "                  category: str,\n",
    "                  title_tags=(\"h3\", \"h2\"),\n",
    "                  sleep_s: float = 1.0) -> pd.DataFrame:\n",
    "    try:\n",
    "        print(f\"[WEB] Fetching {category}: {url}\")\n",
    "        r = requests.get(url, headers=WEB_HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        print(f\"[WEB] HTTP error for {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "    except RequestException as e:\n",
    "        print(f\"[WEB] Request error for {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    rows = []\n",
    "    seen = set()\n",
    "\n",
    "    for tag_name in title_tags:\n",
    "        for el in soup.find_all(tag_name):\n",
    "            text = el.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            name_clean = clean_name(text)\n",
    "            if len(name_clean) < 3:\n",
    "                continue\n",
    "            if name_clean in seen:\n",
    "                continue\n",
    "            seen.add(name_clean)\n",
    "            rows.append(\n",
    "                {\"Item\": name_clean, \"Category\": normalise_category(category)}\n",
    "            )\n",
    "\n",
    "    time.sleep(sleep_s)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "    print(f\"[WEB] {category}: collected {len(df)} items from {url}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87309589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError, RequestException\n",
    "\n",
    "def scrape_titles_css(url: str, category: str, css_selector: str, sleep_s: float = 1.0) -> pd.DataFrame:\n",
    "\n",
    "    try:\n",
    "        print(f\"[WEB] Fetching {category}: {url}\")\n",
    "        r = requests.get(url, headers=WEB_HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        print(f\"[WEB] HTTP error for {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "    except RequestException as e:\n",
    "        print(f\"[WEB] Request error for {url}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    elements = soup.select(css_selector)\n",
    "\n",
    "    rows = []\n",
    "    seen = set()\n",
    "\n",
    "    for el in elements:\n",
    "        text = el.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        name_clean = clean_name(text)\n",
    "        if len(name_clean) < 3:\n",
    "            continue\n",
    "\n",
    "        if name_clean in seen:\n",
    "            continue\n",
    "        seen.add(name_clean)\n",
    "\n",
    "        rows.append(\n",
    "            {\"Item\": name_clean, \"Category\": normalise_category(category)}\n",
    "        )\n",
    "\n",
    "    time.sleep(sleep_s)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "    print(f\"[WEB] {category}: collected {len(df)} items from {url} ({len(df)} unique)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06aae8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pets_at_home() -> pd.DataFrame:\n",
    "    # Build Pet Supplies dataset from Pets at Home\n",
    "    urls = [\n",
    "        \"https://www.petsathome.com/product/listing/dog/dog-food\",\n",
    "        \"https://www.petsathome.com/product/listing/cat/cat-food/dry-cat-food\",\n",
    "        \"https://www.petsathome.com/product/listing/fish/fish-food\",\n",
    "        \"https://www.petsathome.com/product/listing/bird-and-wildlife/wildbird-food\",\n",
    "        \"https://www.petsathome.com/product/listing/small-animal/rabbit/rabbit-food-and-feeding-hay\",\n",
    "        \"https://www.petsathome.com/product/listing/small-animal/hamster/hamster-food\",\n",
    "    ]\n",
    "\n",
    "    dfs = []\n",
    "    for url in urls:\n",
    "        # product titles: <h3 class=\"product-info_title__2XVM2\">\n",
    "        df_cat = scrape_titles_css(\n",
    "            url,\n",
    "            category=\"Pet Supplies\",\n",
    "            css_selector=\"h3.product-info_title__2XVM2\",\n",
    "        )\n",
    "        if not df_cat.empty:\n",
    "            dfs.append(df_cat)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_eataly_food() -> pd.DataFrame:\n",
    "    # Build food categories (Pasta & Grains, Condiments & Sauces, Canned Goods) from Eataly.\n",
    "    url_map = {\n",
    "        \"Pasta & Grains\": \"https://www.eataly.com/us_en/nationwide-shipping/pasta\",\n",
    "        \"Condiments & Sauces\": \"https://www.eataly.com/us_en/nationwide-shipping/pantry/salt-and-spices\",\n",
    "        \"Canned Goods\": \"https://www.eataly.com/us_en/nationwide-shipping/pantry/canned-goods\",\n",
    "    }\n",
    "\n",
    "    dfs = []\n",
    "    for cat, url in url_map.items():\n",
    "        # product titles: <div class=\"product-card-name\" data-test-e2e=\"product-card-product-name\">\n",
    "        df_cat = scrape_titles_css(\n",
    "            url,\n",
    "            category=cat,\n",
    "            css_selector='div.product-card-name[data-test-e2e=\"product-card-product-name\"]',\n",
    "        )\n",
    "        if not df_cat.empty:\n",
    "            dfs.append(df_cat)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7244cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575    wainwright's sensitive adult dry dog food atla...\n",
      "576    wainwright's complete adult dry dog food salmo...\n",
      "577    wainwright's dry adult dog food beef with supe...\n",
      "578    wainwright's complete adult dry dog food turke...\n",
      "579    hill's science plan sensitive stomach & skin m...\n",
      "580    hill's science plan adult medium breed dry dog...\n",
      "581    hill's science plan perfect weight large breed...\n",
      "582    hill's science plan perfect weight medium bree...\n",
      "583    james wellbeloved hypoallergenic senior dry do...\n",
      "584    james wellbeloved hypoallergenic adult dry dog...\n",
      "585    james wellbeloved adult dry dog food chicken &...\n",
      "586    james wellbeloved senior dry dog food lamb & rice\n",
      "587            ava sensitive skin & stomach dry dog food\n",
      "588          ava medium breed adult dry dog food chicken\n",
      "589           ava large breed adult dry dog food chicken\n",
      "590           ava small breed adult dry dog food chicken\n",
      "591    pro plan medium everyday nutrition adult dry d...\n",
      "592    pro plan sensitive skin medium breed adult dry...\n",
      "593    pro plan all sizes light neutered adult dry do...\n",
      "594    pro plan medium puppy healthy start dry dog fo...\n",
      "595    arden grange adult dry dog food with chicken &...\n",
      "596    arden grange sensitive grain free adult dry do...\n",
      "597    arden grange senior dry dog food with chicken ...\n",
      "598    arden grange large breed adult dry dog food wi...\n",
      "599    step up naturals no added grain adult dry dog ...\n",
      "600    step up naturals mature dry dog food chicken w...\n",
      "601    step up naturals no added grain adult dry dog ...\n",
      "602      step up naturals dry adult dog food beef & lamb\n",
      "603    wildways dry adult dog food fresh beef with bu...\n",
      "604    wildways dry adult dog food fresh duck with ch...\n",
      "Name: Item, dtype: object\n",
      "546                    smoky spanish-style grains & rice\n",
      "547                              wholegrain basmati rice\n",
      "548                     good grains spanish style grains\n",
      "549                                    pure basmati rice\n",
      "550                                  basmati & wild rice\n",
      "551                              classic wholegrain rice\n",
      "552                               tomato and basil sauce\n",
      "553                                             linguine\n",
      "554                                            spaghetti\n",
      "555                                             bucatini\n",
      "556    limited edition chitarra spaghetti by chef car...\n",
      "557                  100 organic italian grain spaghetti\n",
      "558                                pappardelle egg pasta\n",
      "559                                              busiate\n",
      "560                           100 italian grain bucatini\n",
      "561                           100 italian grain linguine\n",
      "562                           100 italian grain rigatoni\n",
      "563                100 organic italian grain tortiglioni\n",
      "564                                    super spaghettoni\n",
      "565                          100 italian grain spaghetti\n",
      "566                          100 italian grain caserecce\n",
      "567                                             rigatoni\n",
      "568                    100 organic italian grain vesuvio\n",
      "569                                lemon pepper linguine\n",
      "570                                linguine with truffle\n",
      "571                                gluten-free spaghetti\n",
      "572                                           pici pasta\n",
      "573                                      trofiette pasta\n",
      "574                            spaghetti squid ink pasta\n",
      "Name: Item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[df[\"Category\"] == \"Pet Supplies\"][\"Item\"].head(30))\n",
    "print(df[df[\"Category\"] == \"Pasta & Grains\"][\"Item\"].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d943684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all scraped data\n",
    "def build_web_dataset() -> pd.DataFrame:\n",
    "    # combine all web-based sources that are accessible: Pets at Home -> Pet Supplies, Boots -> Personal Care, Eataly -> Pasta & Grains, Condiments & Sauces, Canned Goods\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    df_pets = build_pets_at_home()\n",
    "    if not df_pets.empty:\n",
    "        dfs.append(df_pets)\n",
    "\n",
    "    df_boots = build_boots_personal_care()\n",
    "    if not df_boots.empty:\n",
    "        dfs.append(df_boots)\n",
    "\n",
    "    df_eataly = build_eataly_food()\n",
    "    if not df_eataly.empty:\n",
    "        dfs.append(df_eataly)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"[WEB] No web data collected.\")\n",
    "        return pd.DataFrame(columns=[\"Item\", \"Category\"])\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65eb3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebalance ccategories and cap at 600\n",
    "def rebalance_by_cap(df: pd.DataFrame, cap: int = 600) -> pd.DataFrame:\n",
    "    groups = []\n",
    "    for cat, g in df.groupby(\"Category\", group_keys=False):\n",
    "        if len(g) > cap:\n",
    "            g = g.sample(cap, random_state=42)\n",
    "        groups.append(g)\n",
    "    return pd.concat(groups, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01043938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OFF] Page 1/10: +100 products, dataset size now 92\n",
      "[OFF] Page 2/10: +100 products, dataset size now 185\n",
      "[OFF] Page 3/10: +100 products, dataset size now 274\n",
      "[OFF] Page 4/10: +100 products, dataset size now 360\n",
      "[OFF] Page 5/10: +100 products, dataset size now 446\n",
      "[OFF] Page 6/10: +100 products, dataset size now 533\n",
      "[OFF] Page 7/10: +100 products, dataset size now 622\n",
      "[OFF] Page 8/10: +100 products, dataset size now 711\n",
      "[OFF] Page 9/10: +100 products, dataset size now 795\n",
      "[OFF] Page 10/10: +100 products, dataset size now 878\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/dog/dog-food\n",
      "[WEB] Pet Supplies: collected 40 items from https://www.petsathome.com/product/listing/dog/dog-food (40 unique)\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/cat/cat-food/dry-cat-food\n",
      "[WEB] Pet Supplies: collected 36 items from https://www.petsathome.com/product/listing/cat/cat-food/dry-cat-food (36 unique)\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/fish/fish-food\n",
      "[WEB] Pet Supplies: collected 37 items from https://www.petsathome.com/product/listing/fish/fish-food (37 unique)\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/bird-and-wildlife/wildbird-food\n",
      "[WEB] Pet Supplies: collected 37 items from https://www.petsathome.com/product/listing/bird-and-wildlife/wildbird-food (37 unique)\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/small-animal/rabbit/rabbit-food-and-feeding-hay\n",
      "[WEB] Pet Supplies: collected 36 items from https://www.petsathome.com/product/listing/small-animal/rabbit/rabbit-food-and-feeding-hay (36 unique)\n",
      "[WEB] Fetching Pet Supplies: https://www.petsathome.com/product/listing/small-animal/hamster/hamster-food\n",
      "[WEB] Pet Supplies: collected 12 items from https://www.petsathome.com/product/listing/small-animal/hamster/hamster-food (12 unique)\n",
      "[WEB] Fetching Personal Care: https://www.boots.com/toiletries/washing-bathing/soap-hand-wash\n",
      "[WEB] Personal Care: collected 0 items from https://www.boots.com/toiletries/washing-bathing/soap-hand-wash (0 unique)\n",
      "[WEB] Fetching Pasta & Grains: https://www.eataly.com/us_en/nationwide-shipping/pasta\n",
      "[WEB] Pasta & Grains: collected 23 items from https://www.eataly.com/us_en/nationwide-shipping/pasta (23 unique)\n",
      "[WEB] Fetching Condiments & Sauces: https://www.eataly.com/us_en/nationwide-shipping/pantry/salt-and-spices\n",
      "[WEB] Condiments & Sauces: collected 18 items from https://www.eataly.com/us_en/nationwide-shipping/pantry/salt-and-spices (18 unique)\n",
      "[WEB] Fetching Canned Goods: https://www.eataly.com/us_en/nationwide-shipping/pantry/canned-goods\n",
      "[WEB] Canned Goods: collected 23 items from https://www.eataly.com/us_en/nationwide-shipping/pantry/canned-goods (23 unique)\n",
      "Saved: data/off_grocery_dataset.csv\n",
      "Category\n",
      "Pet Supplies           197\n",
      "Snacks                 157\n",
      "Bakery                 150\n",
      "Condiments & Sauces     95\n",
      "Dairy & Eggs            94\n",
      "Beverages               80\n",
      "Produce                 51\n",
      "Canned Goods            43\n",
      "Meat & Seafood          29\n",
      "Pasta & Grains          29\n",
      "Frozen Foods            28\n",
      "Household               27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# build final dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Open Food Facts\n",
    "    df_off = build_off_dataset(\n",
    "        country=\"united-kingdom\",\n",
    "        pages=10,\n",
    "        page_size=500,\n",
    "        sleep_s=1.0,\n",
    "    )\n",
    "\n",
    "    # drop \"Other\" from OFF to only take confident labels\n",
    "    df_off = df_off[df_off[\"Category\"] != \"Other\"]\n",
    "\n",
    "    # 2. Web datasets (Pets at Home, Boots, Eataly)\n",
    "    df_web = build_web_dataset()\n",
    "\n",
    "    # 3. Manual items\n",
    "    manual_path = \"data/extra_manual_items.csv\"\n",
    "    df_manual = pd.read_csv(manual_path)\n",
    "    df_manual[\"Item\"] = df_manual[\"Item\"].apply(clean_name)\n",
    "    df_manual[\"Category\"] = df_manual[\"Category\"].apply(normalise_category)\n",
    "\n",
    "    # 4. Combine everything\n",
    "    df = pd.concat([df_off, df_web, df_manual], ignore_index=True)\n",
    "\n",
    "    # normalise categories + filter to the ones you actually use\n",
    "    df[\"Category\"] = df[\"Category\"].apply(normalise_category)\n",
    "    keep = {\n",
    "        \"Produce\", \"Meat & Seafood\", \"Dairy & Eggs\", \"Bakery\", \"Pantry\",\n",
    "        \"Frozen Foods\", \"Beverages\", \"Snacks\", \"Personal Care\", \"Household\",\n",
    "        \"Pet Supplies\", \"Deli\", \"Condiments & Sauces\", \"Canned Goods\", \"Pasta & Grains\",\n",
    "    }\n",
    "    df = df[df[\"Category\"].isin(keep)]\n",
    "\n",
    "    # 5. Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"Item\", \"Category\"]).reset_index(drop=True)\n",
    "\n",
    "    # 6. Cap large categories so they don't dominate\n",
    "    df = rebalance_by_cap(df, cap=600)\n",
    "\n",
    "    # 7. Save\n",
    "    out_path = \"data/off_grocery_dataset.csv\"  # same name as before\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    print(df[\"Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "975f8021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Pet Supplies           197\n",
       "Snacks                 157\n",
       "Bakery                 150\n",
       "Condiments & Sauces     95\n",
       "Dairy & Eggs            94\n",
       "Beverages               80\n",
       "Produce                 51\n",
       "Canned Goods            43\n",
       "Meat & Seafood          29\n",
       "Pasta & Grains          29\n",
       "Frozen Foods            28\n",
       "Household               27\n",
       "Name: Item, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"Category\"].value_counts()\n",
    "df.groupby(\"Category\")[\"Item\"].nunique().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding-five",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
